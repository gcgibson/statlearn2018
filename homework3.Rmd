---
title: "Homework 3"
author: "Casey Gibson"
date: "3/1/2018"
output:
  pdf_document: default
  html_document: default
---
### Problem 1

#### a
Best subset selection will have the smallest training RSS, because it is able to search accross all models to choose the best one. Forward and backward selection may have the same training RSS, but not lower.

#### b

Since any of these selection techniques are prone to overfitting there is no way to know a-priori which will have the lowest test RSS.

#### c

##### i
True - forward selection can only add predictors to the existing $k$-variable model.

##### ii
True - the $k$-variable is obtained by removing a variable from the $k+1$-variable model.

##### iii
False - No link exists between forward and backward selection.

##### iv
False - see above

##### v
False - Best subset selection is allowed to vary all predictors when going from $k$-variable to $k+1$-variable models.


### Problem 2

#### a)
iii - Less flexible because we have imposed a constraint on the $\beta$s and improved prediction accuracy. This constraint also reduces the variance, and as long as the bias increase is smaller than the variance decrease, the MSE will go down.

#### b)
iii - Less flexible because we have imposed a constraint on the $\beta$s and improved prediction accuracy. This constraint also reduces the variance, and as long as the bias increase is smaller than the variance decrease, the MSE will go down.

### Problem 3

#### a)
iv - As we relax the constraints on $\beta$ the model is more flexible so we expect a decrease on the training data. 

#### b)
ii -  As we relax the constraints on $\beta$ the model is more flexible so we expect a decrease on the testing data at first, until we overfit. 


#### c)
iii - Variance increases as the model becomes more flexible because we have more possible $\beta$ values with positive probability. 

#### d)
iv - Bias decreases as the model becomes more flexible, since the OLS estimate is unbiased and the fully unconstrained estimate = OLS. 

### Problem 4

$$\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2$$
We can add and subtract $0$ in the following way 

$$\sum_{i=1}^n (y_i - \beta_0 -\sum_{j=1}^p \bar{x_j}\beta_j + \sum_{j=1}^p \bar{x_j}\beta_j - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2$$
$$\sum_{i=1}^n (y_i - (\beta_0+\sum_{j=1}^p \bar{x_j}\beta_j ) - \sum_{j=1}^p (x_{ij} -\bar{x_j})\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2$$
So we can see from this that 

$$\beta_0^c = \beta_0 + \sum_{j=1}^p \bar{x_j}\beta_j$$
and all other $\beta_j^c = \beta_j \ for \ j \neq 0$.

And that by minimizing the first, we are indeed minimizing the second. We could have also shown this using the partial derivative hint, but this seems easier.




### Problem 5

#### a

```{r}
set.seed(1)
p = 20
n = 1000

beta <- c(rep(1,5),rep(0,15))
xs <- matrix(rnorm(n*p,0,sqrt(2)),nrow=n,ncol=p) 
error <- rnorm(n,0,sqrt(2))

y <- xs%*%beta + error

```

#### b
```{r}
train_indexes <- sample(n,size=500,replace = FALSE)
train_y <- y[train_indexes]
test_y <- y[-train_indexes]

train_x <- xs[train_indexes,]
test_x <- xs[-train_indexes,]
```

#### c

```{r}
library(ISLR)
library(leaps)

data.train =data.frame(y = train_y,x = train_x)
data.test =data.frame(y = test_y,x = test_x)

regfit = regsubsets(y ~. ,data=data.train,nvmax=20)

data.train$"(Intercept)" <- rep(0,500) 
errors <- rep(NA,20)

for (i in 1:20){
  coef = coef(regfit, id=i)
  pred <- as.matrix(data.train[,names(coef)])%*%matrix(coef,ncol=1)
 # print (pred)
  errors[i]=mean((data.train$y-pred)^2)
}

plot(errors,type="b",main="Training MSE",xlab="Num Predictors",ylab="MSE",pch=19,ylim=c(0,15))


```

#### d
```{r}
library(ISLR)
library(leaps)

data.train =data.frame(y = train_y,x = train_x)
regfit = regsubsets(y ~. ,data=data.train,nvmax=20)

data.test$"(Intercept)" <- rep(1,500) 
errors <- rep(NA,20)

for (i in 1:20){
  coef = coef(regfit, id=i)
  pred <- as.matrix(data.test[,names(coef)])%*%matrix(coef,ncol=1)
  errors[i]=mean((data.test$y-pred)^2)
}

plot(errors,type="b",pch=19,main="Testing MSE",xlab="Num Predictors",ylab="MSE",ylim=c(0,15))
```

#### e

```{r}

print (which.min(errors))
```

We can see that the model with $5$ predictors has the lowest test MSE, which is consistent with the true underlying data generating mechanism since we selected 5 active predictors to generate the data. 


#### f

```{r}
print (coef(regfit, id=20))
```

We can see the coefficients for the first 5 predictors are much higher than those of the remaining 15, which is again consistent with how we generated the data. 

### Problem 6

#### a

```{r, echo=FALSE,message=FALSE, warning=FALSE}
library(MASS)
require(glmnet)  # ridge and lasso
grid=10^seq(10,-2,length=100)
data(Boston)
set.seed(1)

x <- model.matrix(crim~., data=Boston)[,-1]
y <- Boston$crim
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)

set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min

lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])

out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)
print (bestlam)
lasso.coef
```

#### b 
```{r}


set.seed(1)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)

cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
predict(out,type="coefficients",s=bestlam)


```

#### c 

```{r}

print (mean((lasso.pred-y.test)^2))
print (mean((ridge.pred-y.test)^2))
```
#### d

We see that both age and tax are zeroed out in the lasso model, which has a lower MSE than ridge. 
