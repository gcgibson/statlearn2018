---
title: "Homework 3"
author: "Casey Gibson"
date: "3/1/2018"
output: html_document
---
### Problem 1

#### a
Best subset selection will have the smallest training RSS, because it is able to search accross all models to choose the best one. Forward and backward selection may have the same training RSS, but not lower.

#### b

Since any of these selection techniques are prone to overfitting there is no way to know a-priori which will have the lowest test RSS.

#### c

##### i
True

##### ii
True

##### iii
False

##### iv
False

##### v
False


### Problem 2

#### a)
iii

#### b)
iii 

### Problem 3

#### a)
iv

#### b)
ii

#### c)
iii

#### d)
iv

### Problem 4

$$\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2$$
We can add and subtract $0$ in the following way 

$$\sum_{i=1}^n (y_i - \beta_0 -\sum_{j=1}^p \bar{x_j}\beta_j + \sum_{j=1}^p \bar{x_j}\beta_j - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2$$
$$\sum_{i=1}^n (y_i - (\beta_0+\sum_{j=1}^p \bar{x_j}\beta_j ) - \sum_{j=1}^p (x_{ij} -\bar{x_j})\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2$$
So we can see from this that 

$$\beta_0^c = \beta_0 + \sum_{j=1}^p \bar{x_j}\beta_j$$
and all other $\beta_j^c = \beta_j \ for \ j \neq 0$.

And that by minimizing the first, we are indeed minimizing the second. We could have also shown this using the partial derivative hint, but this seems easier.




### Problem 5

#### a

```{r}
set.seed(1)
p = 20
n = 1000

beta <- c(rep(1,5),rep(0,15))
xs <- matrix(rnorm(n*p,0,sqrt(2)),nrow=n,ncol=p) 
error <- rnorm(n,0,sqrt(2))

y <- xs%*%beta + error

head(y)
```

#### b
```{r}
train_indexes <- sample(n,size=500,replace = FALSE)
train_y <- y[train_indexes]
test_y <- y[-train_indexes]

train_x <- xs[train_indexes,]
test_x <- xs[-train_indexes,]
```

#### c

```{r}
library(ISLR)
library(leaps)

data.train =data.frame(y = train_y,x = train_x)
data.test =data.frame(y = test_y,x = test_x)

regfit = regsubsets(y ~. ,data=data.train,nvmax=20)

data.train$"(Intercept)" <- rep(0,500) 
errors <- rep(NA,20)

for (i in 1:20){
  coef = coef(regfit, id=i)
  pred <- as.matrix(data.train[,names(coef)])%*%matrix(coef,ncol=1)
  print (pred)
  errors[i]=mean((data.train$y-pred)^2)
}

plot(errors)


```

#### d
```{r}
library(ISLR)
library(leaps)

data.train =data.frame(y = train_y,x = train_x)
regfit = regsubsets(y ~. ,data=data.train,nvmax=20)

data.test$"(Intercept)" <- rep(1,500) 
errors <- rep(NA,20)

for (i in 1:20){
  coef = coef(regfit, id=i)
  pred <- as.matrix(data.test[,names(coef)])%*%matrix(coef,ncol=1)
  errors[i]=mean((data.test$y-pred)^2)
}

plot(errors,type="b",pch=19)
print (which.min(errors))
```


### Problem 6

#### a

```{r}
library(MASS)
require(glmnet)  # ridge and lasso

data(Boston)
set.seed(1)

xmat <- model.matrix(crim~., data=Boston)
crim_vals <- Boston$crim

# lasso regression model
fit.lasso <- cv.glmnet(xmat, crim_vals, alpha=1)
(lambda <- fit.lasso$lambda.min)  # optimal lambda
pred.lasso <- predict(fit.lasso, s=lambda, newx=xmat.test)
(err.lasso <- mean((test$crim - pred.lasso)^2))  # test error
predict(fit.lasso, s=lambda, type="coefficients")


```


#### b
```{r}

# ridge regression model
fit.ridge <- cv.glmnet(xmat, crim_vals, alpha=0)
(lambda <- fit.ridge$lambda.min)  # optimal lambda
pred.ridge <- predict(fit.ridge, s=lambda, newx=xmat.test)
(err.ridge <- mean((test$crim - pred.ridge)^2))  # test error
predict(fit.ridge, s=lambda, type="coefficients")

```