---
title: "Homework 2"
author: "Casey Gibson"
output:
  pdf_document: default
  html_document: default
header-includes: \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Problem 1 

If we are given $$X_1,X_2,X_3,X_4$$

\textbf{a)}
We know that the number of distinct boostrap samples is given by 
$$\binom{2n-1}{n-1} = \binom{2*4 - 1}{4-1} = 35$$

$$[X_1,X_1,X_3,X_4],[X_1,X_1,X_3,X_3],[X_1,X_1,X_3,X_1],[X_1,X_1,X_3,X_2]$$

\textbf{b)}
If $n=10$ then the probability that $X_1 \in B$ exactly $k$ times is simply binomial 

$$\binom{n}{k} (\frac{1}{10})^{k}(\frac{9}{10})^{10-k}$$

which we can see is 

```{r}
prob1 <- function(n,k) {
  
  return (dbinom(k,n,.1))
}

print (signif(prob1(10,0),3))
print (signif(prob1(10,1),3))
print (signif(prob1(10,2),3))
print (signif(prob1(10,3),3))
print (signif(prob1(10,4),3))
```


\textbf{c)}

```{r}
set.seed(1) #set a random seed
n=10 #sample size (or number of subjects in the original sample)
N=100000 #number of bootstrap replications
store=rep(NA,N)
for(i in 1:N){
      store[i]=sum(sample(1:n, rep=TRUE)==1)
}
store <- table(store)
print (store['0']/N)
print (store['1']/N)
print (store['2']/N)
print (store['3']/N)
print (store['4']/N)
```

### Problem 2
Suppose that $\hat{\phi} = m(\hat{\theta})$ for $m(*)$ is a monotone function. 
$$ P(\hat{\theta}^* \geq \hat{\theta})$$ is by definition 

$$ = \frac{\#(\hat{\theta}^* - \hat{\theta})}{\binom{N}{n}}$$

$$ = \frac{\#(m(\hat{\theta})^* - \hat{m(\theta)})}{\binom{N}{n}}$$  
  
  
  
$$ = ASL_{perm}(\hat{\phi})$$

### Problem 3


Let $$\hat{\phi} = \frac{\hat{\theta}}{\bar{\sigma}\sqrt{\frac{1}{n}+\frac{1}{m}}}$$
In order to show that 

$$\hat{ASL_{perm}}(\hat{\phi}) = \hat{ASL_{perm}}(\hat{\theta})$$

We can use the result from problem $2$ as long as we can show that 

$$m(x) =  \frac{x}{\bar{\sigma}\sqrt{\frac{1}{n}+\frac{1}{m}}}$$

Is a monotonically increasing function.

In order to show this is monotonically increasing, we need to show if

$$x > y \rightarrow m(x) > m(y)$$

I claim that we can re-write $$m(x) = \frac{\bar{z} - \bar{y}}{\sqrt{\frac{[\sum_i (z_i-\bar{z})^2+(y_i-\bar{y})^2]}{n+m-2}}\sqrt{\frac{1}{n}+\frac{1}{m}}}$$ 

as a function in the form $$m(x) = \frac{x}{c(x + a)^2}$$
  
Clearly, we can absorb terms involing $n,m$ into $c$. Since $x = \bar{z} - \bar{y}$ we can write $\bar{z} = x+ \bar{y}$, and similarly for $\bar{y} = x+\bar{z}$, so we see the denominator become..

$$(z_i- x + \bar{z})^2+(y_i  - x+\bar{y})^2$$
  
  which we can factor as some quadratic in $x$, say $(x+a)^2$.
  
  
Now consider $m(x) > m(y)$
$$ \frac{x}{c(x+a)^2 } > \frac{y}{c(y+a)^2 } $$

where we distilled the quadratic function of $\bar{\sigma}$ into an a quadratic function. 

If we expand this out

$$ xc(y-a)^2 > yc(x-a)^2  $$

$$ x(y-a)^2 > y(x-a)^2  $$
$$ x(y^2-2ay +a^2) > y(x^2-2ax + a^2)  $$
$$ xy^2-2xay +xa^2 > yx^2-2ayx + ya^2  $$
$$ xy^2 +xa^2 > yx^2 + ya^2  $$
$$ xa^2 > ya^2  $$

$$ x > y  $$

Thus, starting from $x>y$, we can work backwards and see that $m(x) > m(y)$, therefore $ASL(\hat{\phi}) = ASL(\hat{\theta})$.



### Problem 4

Suppose we have 

$$3.5, 4, 7, 7.3, 8.6, 12.4, 13.8, 18.1$$
the trimmed mean can be calculated as 

$$\frac{1}{4}(7+7.3+8.6+12.4) = 8.825 = \hat{\theta}$$

\textbf{a)}

```{r}
library(boot)

set.seed(1)
data <- c(3.5,4,7,7.3,8.6,12.4,13.8,18.1)

alpha.fn <- function(data,index){
  return (mean(data[index],trim=.25 ))
}
for(i in c(25,100,200,500,1000,2000)){ 
  bt.results=boot(data,alpha.fn,R=i)
  print (c(i,sd(bt.results$t)))
}

```

\textbf{b)}

```{r}
library(boot)


ses <- matrix(NA,nrow=10,ncol=6)
data <- c(3.5,4,7,7.3,8.6,12.4,13.8,18.1)

alpha.fn <- function(data,index){
  return (mean(data[index],trim=.25 ))
}

for (j in seq(1,10)){
  set.seed(j)
  count <- 1
  for(i in c(25,100,200,500,1000,2000)){ 
   bt.results=boot(data,alpha.fn,R=i)
    ses[j,count] <- sd(bt.results$t)
    count <- count +1
  }
}
print (ses)

library("reshape2")
library("ggplot2")

colnames_ <- c("Seed 1","Seed 2","Seed 3","Seed 4","Seed 5","Seed 6","Seed 7","Seed 8","Seed 9","Seed 10")
test_data_long <- as.data.frame(t(ses))
colnames(test_data_long) <- colnames_
test_data_long$id <- c(25,100,200,500,1000,2000)

test_data_long <- melt(test_data_long, id="id")  # convert to long format


ggplot(data=test_data_long,
       aes(x=id, y=value, colour=variable)) +
       geom_line()


```


It looks like once we hit above $1500$ bootstrap samples, the results seem to stabilize. 



### Problem 5

```{r}
n=15
data0=as.data.frame(matrix(0, n, 2))
colnames(data0)=c("x", "y")
data0$X=c(44, 13, 2, 57, 36, 6, 79, 84, 25, 13, 24, 24, 15, 20, 7)
data0$Y=c(34, 41, 6, 24, 29, 13, 9, 4, 7, 19, 19, 3, 1, 1, 19)
x=data0$x
y=data0$y



```

\textbf{a)}


```{r}
R=1000

library(ISLR)
#
alpha.fn=function(data, weight, varname){
  weight.2=weight/mean(weight) #standardize the weights
  return(weighted.mean(unlist(data[,varname]),w=weight.2))
}


alpha.est=alpha.fn(data=data0, weight=rep(1, n),varname = "X")






bayesian.boot=function(data, statistic, R, varname){
  data=as.data.frame(data)
  n1=nrow(data)
  bt.est.original=alpha.fn(data, weight=rep(1, n1),varname = varname )
  bt.est=rep(NULL, R)
  for (i in 1:R){
    weight=rexp(n1, rate=1)
    bt.est[i]=alpha.fn(data, weight,varname = varname)
  }
  std=sd(bt.est)
  results=list(t0=bt.est.original, t=bt.est, std=std)
  return(results)
}

bt.results_x=bayesian.boot(data0,alpha.fn,R=1000, varname="X")

print (var(bt.results_x$t))
```

\textbf{b)}

```{r}

bt.results_y=bayesian.boot(data0,alpha.fn,R=1000, varname="Y")

print (var(bt.results_y$t))

```

\textbf{c)}
```{r}

print (var(bt.results_x$t - bt.results_y$t))

print (quantile(bt.results_x$t - bt.results_y$t,probs = c(.05)))


```

$$H_0 =\text{ There is no treatment effect, } \  \bar{X} - \bar{Y}$$
$$H_A =\text{ There is a treatment effect,} \ \bar{X} - \bar{Y} \neq 0$$


We can reject $H_0$ at $\alpha=.05$ level based on the CI above.


\textbf{d}

```{r}
perm.test = function(x, y, N){
  #x: observations in group 1
  #y: observations in group2
  #N: number of random samples from permutation distribution
  z=c(x,y)
  z.sort=sort(z)
  n=length(x)
  m=length(z)
  test.stat=rep(NULL,N)
  for(i in 1:N){
    perm.index=sample(1:m, n)
  perm.x = z.sort[perm.index]
  perm.y = z.sort[-perm.index]
  test.stat[i] = mean(perm.x) - mean(perm.y)
  }
  test.stat
}


obs.test.stat=mean(data0$X)-mean(data0$Y)
test.2=perm.test(x=data0$X,y=data0$Y,N=5000)

p.value.1=sum(test.2>=obs.test.stat)/length(test.2)
p.value.1

hist(test.2, nclass=50, col=8, main="", xlab="permutation test statistic")
abline(v=obs.test.stat, col=2, lty="dashed")
```


\textbf{e)}


```{r}
perm.test = function(x, y, N){
  #x: observations in group 1
  #y: observations in group2
  #N: number of random samples from permutation distribution
  z=c(x,y)
  z.sort=sort(z)
  n=length(x)
  m=length(z)
  test.stat=rep(NULL,N)
  for(i in 1:N){
    perm.index=sample(1:m, n)
  perm.x = z.sort[perm.index]
  perm.y = z.sort[-perm.index]
  test.stat[i] = median(perm.x) - median(perm.y)
  }
  test.stat
}


obs.test.stat=median(data0$X)-median(data0$Y)
test.2=perm.test(x=data0$X,y=data0$Y,N=5000)

p.value.1=sum(test.2>=obs.test.stat)/length(test.2)
p.value.1

hist(test.2, nclass=50, col=8, main="", xlab="permutation test statistic")
abline(v=obs.test.stat, col=2, lty="dashed")

```



### Problem 6

\textbf{a)}
Suppose $$X \sim U(0,\theta)$$

and we draw an iid random sample $$X_1,X_2,...,X_n$$
We can write the likelihood as 

$$L(\theta | X_1,X_2,...,X_n) = \prod_{i=1}^n \frac{1}{\theta} I_{x_i <\theta}$$


This is only nonzero if all $x_i  < \theta$ so the likelihood $>0$ occurs for any $\theta > x_i$. However, since we have a $\theta$ in the denominator, we penalize larger values of $\theta$ so we want to take $\theta = max(X_i)$ in order to achieve the maximum.

\textbf{b)}
We know from a previous homework that the probability $X_j$ is included in a bootstrap sample $[X_1^*,X_2^*,...,X_n^*]$ is $(1+\frac{1}{n})^n$

In particular we can take $X_j =X_{(n)}$, that is the n-th order statistic (max). The only way that $\hat{\theta^*} = \hat{\theta}$ is if $X_{(n)} \in [X_1^*,X_2^*,...,X_n^*]$, therefore we see that 

 $$P(\hat{\theta^*} = \hat{\theta}) = 1-(1-\frac{1}{n})^n $$
 
In the limit this becomes $1-\frac{1}{e}= .632$

\textbf{c)}

```{r}

set.seed(1) #set a random seed
n=50 #sample size
x=runif(n) #this gives the original sample with 50 observations

print (max(x))
```

\textbf{d)}

```{r}

alpha.fn <- function(data,index){
  return (max(data[index] ))
}

boot.res <-boot(x,alpha.fn,2000)
print (sd(boot.res$t))

```

\textbf{e)}

```{r}
pb <- runif(50,min=0,max=max(x))
curr_theta <- max(pb)
b_samples <-c(curr_theta)

for (i in seq(1, 2000)){
  pb <- runif(50,0,curr_theta)
  b_samples <-c(b_samples,max(pb))
}
print (sd(b_samples))
```


\textbf{f)}

```{r}
set.seed(2)
N=2000
theta.hat.vec=rep(NULL,N)
for (i in 1:N){
    x2=runif(n)
      theta.hat.vec[i]=max(x2)
}
print (sd(theta.hat.vec))


par(mfrow = c(1, 3), pty = "s")
hist(boot.res$t,
nclass=50, xlab = "Nonparametric")
hist(b_samples,
nclass=50, xlab = "Parametric")
hist(theta.hat.vec,
nclass=50,, xlab = "Sampling Distribution")

```

Estimates based on the parametric bootstrap are much closer to the truth than the non-parametric bootstrap. This is because of the fact that, in the limit, the nonparametric bootstrap only contains the max $63.2\%$ of the time. 